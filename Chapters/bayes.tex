


\section{Bayes Learning}

Basic formulas:
\begin{itemize}
\item Same notation from now on $P(A \wedge B)=P(A,B)$
\item \textbf{Product Rule}: $P(A \wedge B)=P(A|B)P(B)=P(B|A)P(A)$
\item \textbf{Sum Rule} : $P(A \vee B)= P(A)+P(B)-P(A \wedge B)$
\item \textbf{Theorem of Probability} (also called marginalization): Given some mutaully exclusive events $A_1,...,A_n$ where $\sum_{i=1}^n P(A_j)=1$ we have:
\[P(B) = \sum_{i=1}^n P(B|A_i)P(A_i)\]
\end{itemize}

\paragraph{Bayes Theorem}
Given:
\begin{itemize}
\item $P(h)$ the prior probability of the hypothesis $h$.
\item $P(D)$ the prior probability of the training data $D$.
\end{itemize}
We have:
\[p(h|D)=\frac{P(D|h)P(h)}{p(D)}\]

\subsection{Maximum a Posteriori probability}
When classifying new data we want to assign it the most probable hypothesis. To do so we can use \textbf{Maximum a posteriori} hypothesis $h_{MAP}$:
\[h_{MAP}=\argmax_{h \in H}P(h|D)=\argmax_{h \in H}\frac{P(D|h)P(h)}{p(D)}= \argmax_{h \in H}P(D|h)P(h)\]
where $H$ is the set of all the hypothesis, and from the third equation we have removed the normalizing constant $P(D)$.\\
Moreover if the prior distribution is uniform, i.e. $P(h_i)=P(h_j),\ \forall i,j \in H$ we can use the \textbf{Maximum Likelihood} hypothesis $h_{ML}$ and have:
\[h_{ML}=\argmax_{h \in H} P(D|h)\] 
We can estimate the maximum $h_{MAP}$ by computing $P(h_i|D)$ for every $h_i \in H$ and then get the maximum, \textbf{but} $h_{MAP}$ return the most probable \textit{hypothesis}, not the most probable classification, so, given a new instance $x$, $h_{MAP}(x)$ might not the return the correct classification nor the most probable.

\paragraph{Example}
Let's consider three possible hypothesis $h_1,h_2h_3$ driven from data $D$, where the probability of each hypothesis is:
\[P(h_1|D)=0.4\quad P(h_2|D)=0.3\quad P(h_3|D)=0.3\]
as we can see the $h_{MAP}=h_1$, that is the hypothesis is the most probable in the dataset.\\
Let us now consider a new instance $x$, and the possible classifications by the three hypothesis:
\[h_1(x)=\oplus \quad h_2(x)=\ominus \quad h_3(x)=\ominus \]
It is now obvious that $h_{MAP}(x)=h_1(x)=\oplus$ is not the most probable classification.

\subsection{Bayes Optimal Classifier}
To fix the above mentioned problem consider the following:
\begin{itemize}
\item $C=\lbrace c_1,c_2,...,c_n\rbrace$ is a set of possible classes.
\item $X=\lbrace x_1,x_2,...,x_m\rbrace$ is a set of instances of the dataset $D$
\item $f: X \rightarrow C$ is the target function that maps an instance to a class.
\item $P(c_j|x',h_i)$ is the probability of a new instance $x'$ to  be classified as the class $c_j$ by a hypothesis $h_i$.
\item $P(c_j| x',D)$ the probability that $x'$ belongs to the class $c_j$ conditioned to the entire dataset $D$ (every hypothesis).
\end{itemize}
We have that:
\[P(c_j| x',D)= \sum_{h_i \in H} P(c_j|x',h_i)P(h_i|D)\]
Moreover this kind of function $f$ is called \textbf{Bayes Optimal Classifier} [OB], so the most probable class $c_{OB}$ for a new instance $x'$ would be:
\[c_{OB}=\argmax_{c_i \in C}\sum_{h_j \in H}P(c_i|x',h_j)P(h_j|D)\]

\paragraph{Example}
Given:
\begin{table}[H]
    \begin{tabular}{|l|l|l|}
        \hline
        hypothesis prior & positive class    & negative class     \\ \hline
        $P(h_1|D)=0.4 $    &  $P(\oplus|x,h_1)=0$ &$ P(\ominus|x,h_1)=1$ \\ 
        $P(h_2|D)=0.3  $   & $P(\oplus|x,h_2)=1$ & $P(\ominus|x,h_2)=0$ \\ 
        $P(h_3|D)=0.3$     &$P(\oplus|x,h_3)=1 $& $P(\ominus|x,h_3)=0 $\\
        \hline
    \end{tabular}
\end{table}
We have that:
\[\sum_{h_i \in H} P(\oplus|x',h_i)P(h_i|D)=0.4\]
\[\sum_{h_i \in H} P(\ominus|x',h_i)P(h_i|D)=0.6\]
Thus:
\[c_{OB}=\argmax_{c_i \in C}\sum_{h_j \in H}P(c_i|x',h_j)P(h_j|D)=\ominus\]

\section{Other Distributions}

\subsection{Bernoulli}
Describes the probability distribution of a binary random variable $X \in \lbrace 0,1 \rbrace$:
\[P(X=1)=\theta \quad P(X=0)=1-\theta\]
The probability of $X$ being a certain value $x$, given $\theta$ is:
\[P(X=x,\theta)=\theta^x(1-\theta)^{1-x}\]
While the probability of obtaining $k$ times the same result with $n$ trials is:
\[P(X=k,n,\theta)=\binom{n}{k} \theta^k(1-\theta)^{n-k}\]

\paragraph{Multi-variate Bernoulli distribution}
Having a set of mutually independent binary random variable $X_1,X_2,...,X_n$ following a Bernoulli distribution, the Multi-variate Bernoulli distribution [MvBd] is the product of all $n$ distributions:
\[\prod_{i=1}^n P(X_i=x_i,\theta_i)\]


\section{Naive Bayes Classifier}
When we have to deal with a high hypothesis space, the Bayes Optimal Classifier  is not practical anymore. A way to avoid computing every hypothesis is using conditional independence.\\
Let's write a new instance $x'$ as a set of attributes $A=\lbrace a_1,a_2,...,a_n\rbrace$, so we have that:
\[c_{OB}=P(c_j| x',D)=P(c_j| A,D)=P(c_j| a_1,a_2,...,a_n,D)\]
Using the Bayes theorem we have:
\[c_{OB}=\argmax P(c_j| A,D)=\argmax \frac{P(A|c_j,D)P(c_j|D)}{P(A|D)}=\argmax P(A|c_j,D)P(c_j|D)\]
In which $P(A|c_j,D)$ is too difficult to compute.\\
For this reason the Naive classifier assumes that each attribute is independent from the other, effectively transforming $P(a_1,a_2,...,a_n|c_j,D)$ to $\prod_i P(a_i|c_j,D)$, so that the $f_{NB}$ becomes:
\[c_{NB}=\argmax_{c_j \in C} P(c_j|D)\prod_i P(a_i|c_j,D)\]\\

It can happen during estimation that some attributes $a_i$ does not have a prior for a class $c_j$ so that $P(a_i|c_j,D)=0 \Rightarrow P(c_j|D)\prod_i P(a_i|c_j,D)=0$. In this case we can set a \textit{virtual prior} to some arbitrary number to avoid having zero.

